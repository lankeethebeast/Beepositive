{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ Bee Positive: UK Biodiversity Indicators â€” Machine Learning Analysis\n",
    "---\n",
    "## 1. Introduction & Project Overview\n",
    "\n",
    "**Client**: Bee Positive (Non-Government Organisation)  \n",
    "**Objective**: Develop a proof-of-concept ML application to analyse UK Biodiversity Indicator datasets, with the goal of planning, forecasting, and evaluating actions to support pollinating insect populations and biodiversity.\n",
    "\n",
    "### Datasets Analysed:\n",
    "1. **Pollinating Insects** â€” Occupancy indices for bees & hoverflies (1980â€“2024)\n",
    "2. **Habitat Connectivity** â€” Functional connectivity indices for butterflies & birds (1985â€“2012)\n",
    "3. **Agri-Environment Schemes** â€” Land area under environmental schemes by country (1992â€“2022)\n",
    "4. **Plants of the Wider Countryside** â€” Plant abundance indices by habitat type (2015â€“2024)\n",
    "5. **Insects of the Wider Countryside** â€” Butterfly abundance trends (1976â€“2024)\n",
    "\n",
    "### Methodology:\n",
    "- **AI Search/Optimisation**: Genetic Algorithm for optimal data imputation strategy selection\n",
    "- **Supervised ML**: Regression models to predict pollinator population trends\n",
    "- **Unsupervised ML**: K-Means clustering to identify species groupings by trend patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup & Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (mean_absolute_error, mean_squared_error, r2_score,\n",
    "                             silhouette_score, silhouette_samples)\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"All libraries imported successfully.\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Initial Exploration\n",
    "\n",
    "We load all five datasets from their Excel files. The Plants dataset requires a custom XML parser due to a file format incompatibility with the openpyxl library â€” the workbook uses a non-standard OOXML namespace declaration, which is a data quality challenge we must address.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define file paths\n",
    "BASE_PATH = '.'\n",
    "FILES = {\n",
    "    'pollinating': 'UK-BDI-2025-pollinating-insects.xlsx',\n",
    "    'habitat': 'UK-BDI-2025-habitat-connectivity.xlsx',\n",
    "    'agri': 'UK-BDI-2025-agri-environment-schemes.xlsx',\n",
    "    'plants': 'UK-BDI-2025-plants-wider-countryside.xlsx',\n",
    "    'insects': 'UK-BDI-2025-insects-wider-countryside.xlsx'\n",
    "}\n",
    "\n",
    "# Verify all files exist\n",
    "for key, fname in FILES.items():\n",
    "    path = os.path.join(BASE_PATH, fname)\n",
    "    exists = os.path.exists(path)\n",
    "    size = os.path.getsize(path) if exists else 0\n",
    "    print(f\"{'âœ…' if exists else 'âŒ'} {key}: {fname} ({size:,} bytes)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Pollinating Insects Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Sheet 1: Yearly occupancy index for ALL pollinators (1980-2024)\n",
    "poll_yearly = pd.read_excel(os.path.join(BASE_PATH, FILES['pollinating']), sheet_name='1', header=None)\n",
    "poll_yearly = poll_yearly.iloc[3:].reset_index(drop=True)\n",
    "poll_yearly.columns = ['Year', 'Indicator_Mean_Occupancy', 'CI_Lower', 'CI_Upper']\n",
    "poll_yearly = poll_yearly.apply(pd.to_numeric, errors='coerce')\n",
    "print(\"Pollinating Insects - Yearly Occupancy Index (All Pollinators):\")\n",
    "print(f\"Shape: {poll_yearly.shape}\")\n",
    "display(poll_yearly.head(10))\n",
    "print(f\"\\nYear range: {poll_yearly['Year'].min():.0f} to {poll_yearly['Year'].max():.0f}\")\n",
    "print(f\"Missing values:\\n{poll_yearly.isnull().sum()}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Sheet 2: Long/short term trend summary (all pollinators)\n",
    "poll_trends = pd.read_excel(os.path.join(BASE_PATH, FILES['pollinating']), sheet_name='2', header=None)\n",
    "poll_trends = poll_trends.iloc[3:].reset_index(drop=True)\n",
    "poll_trends.columns = ['Trend', 'Period', 'Num_Species', 'Pct_Species']\n",
    "poll_trends[['Num_Species', 'Pct_Species']] = poll_trends[['Num_Species', 'Pct_Species']].apply(pd.to_numeric, errors='coerce')\n",
    "print(\"Pollinator Trend Summary (All Pollinators):\")\n",
    "display(poll_trends)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Sheet 3: Yearly occupancy for WILD BEES\n",
    "poll_bees = pd.read_excel(os.path.join(BASE_PATH, FILES['pollinating']), sheet_name='3', header=None)\n",
    "poll_bees = poll_bees.iloc[3:].reset_index(drop=True)\n",
    "poll_bees.columns = ['Year', 'Bee_Occupancy', 'Bee_CI_Lower', 'Bee_CI_Upper']\n",
    "poll_bees = poll_bees.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Sheet 5: Yearly occupancy for HOVERFLIES\n",
    "poll_hover = pd.read_excel(os.path.join(BASE_PATH, FILES['pollinating']), sheet_name='5', header=None)\n",
    "poll_hover = poll_hover.iloc[3:].reset_index(drop=True)\n",
    "poll_hover.columns = ['Year', 'Hoverfly_Occupancy', 'Hoverfly_CI_Lower', 'Hoverfly_CI_Upper']\n",
    "poll_hover = poll_hover.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Sheet 7: Species list\n",
    "poll_species = pd.read_excel(os.path.join(BASE_PATH, FILES['pollinating']), sheet_name='7', header=None)\n",
    "poll_species = poll_species.iloc[3:].reset_index(drop=True)\n",
    "poll_species.columns = ['Taxon_Group', 'Species']\n",
    "\n",
    "print(f\"Wild Bees yearly data: {poll_bees.shape} | Year range: {poll_bees['Year'].min():.0f}-{poll_bees['Year'].max():.0f}\")\n",
    "print(f\"Hoverflies yearly data: {poll_hover.shape} | Year range: {poll_hover['Year'].min():.0f}-{poll_hover['Year'].max():.0f}\")\n",
    "print(f\"\\nSpecies list: {len(poll_species)} species\")\n",
    "print(f\"Taxon groups: {poll_species['Taxon_Group'].value_counts().to_dict()}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Habitat Connectivity Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Sheet 1: Composite butterfly connectivity index (1985-2012)\n",
    "hab_butterfly = pd.read_excel(os.path.join(BASE_PATH, FILES['habitat']), sheet_name='1', header=None)\n",
    "hab_butterfly = hab_butterfly.iloc[3:].reset_index(drop=True)\n",
    "hab_butterfly.columns = ['Year', 'Unsmoothed_Index', 'Smoothed_Index', 'Smoothed_CI_Lower', 'Smoothed_CI_Upper']\n",
    "hab_butterfly = hab_butterfly.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Sheet 4: Composite bird connectivity index (1985-2012)\n",
    "hab_bird = pd.read_excel(os.path.join(BASE_PATH, FILES['habitat']), sheet_name='4', header=None)\n",
    "hab_bird = hab_bird.iloc[4:].reset_index(drop=True)\n",
    "hab_bird.columns = ['Year', 'Bird_Unsmoothed', 'Bird_Smoothed', 'Bird_CI_Lower', 'Bird_CI_Upper']\n",
    "hab_bird = hab_bird.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Sheet 3: Individual butterfly species connectivity trends\n",
    "hab_species = pd.read_excel(os.path.join(BASE_PATH, FILES['habitat']), sheet_name='3', header=None)\n",
    "hab_species = hab_species.iloc[4:].reset_index(drop=True)\n",
    "hab_species.columns = ['Common_Name', 'Species_Name', 'Long_Term_Trend', 'Early_Short_Term', 'Late_Short_Term']\n",
    "\n",
    "# Sheet 7 & 8: Mean habitat connectivity values\n",
    "hab_woodland = pd.read_excel(os.path.join(BASE_PATH, FILES['habitat']), sheet_name='7', header=None)\n",
    "hab_woodland = hab_woodland.iloc[3:].reset_index(drop=True)\n",
    "hab_woodland.columns = ['Year', 'Habitat_Type', 'Mean_Connectivity']\n",
    "\n",
    "hab_scotland = pd.read_excel(os.path.join(BASE_PATH, FILES['habitat']), sheet_name='8', header=None)\n",
    "hab_scotland = hab_scotland.iloc[3:].reset_index(drop=True)\n",
    "hab_scotland.columns = ['Year', 'Habitat_Type', 'Mean_Connectivity']\n",
    "\n",
    "print(\"Habitat Connectivity - Butterfly Composite Index:\")\n",
    "print(f\"Shape: {hab_butterfly.shape} | Year range: {hab_butterfly['Year'].min():.0f}-{hab_butterfly['Year'].max():.0f}\")\n",
    "display(hab_butterfly.head())\n",
    "print(f\"\\nBird Composite: {hab_bird.shape}\")\n",
    "print(f\"Individual butterfly species: {len(hab_species)} species\")\n",
    "print(f\"Missing values (butterfly): {hab_butterfly.isnull().sum().to_dict()}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Agri-Environment Schemes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Sheet 1: Higher-level schemes\n",
    "agri_higher = pd.read_excel(os.path.join(BASE_PATH, FILES['agri']), sheet_name='1', header=None)\n",
    "agri_higher = agri_higher.iloc[5:].reset_index(drop=True)\n",
    "agri_higher.columns = ['Year', 'Country', 'Area_MHa']\n",
    "agri_higher['Year'] = pd.to_numeric(agri_higher['Year'], errors='coerce')\n",
    "agri_higher['Area_MHa'] = pd.to_numeric(agri_higher['Area_MHa'], errors='coerce')\n",
    "agri_higher = agri_higher.dropna(subset=['Year'])\n",
    "\n",
    "# Sheet 2: Entry-level schemes\n",
    "agri_entry = pd.read_excel(os.path.join(BASE_PATH, FILES['agri']), sheet_name='2', header=None)\n",
    "agri_entry = agri_entry.iloc[5:].reset_index(drop=True)\n",
    "agri_entry.columns = ['Year', 'Country', 'Area_MHa']\n",
    "agri_entry['Year'] = pd.to_numeric(agri_entry['Year'], errors='coerce')\n",
    "agri_entry['Area_MHa'] = pd.to_numeric(agri_entry['Area_MHa'], errors='coerce')\n",
    "agri_entry = agri_entry.dropna(subset=['Year'])\n",
    "\n",
    "print(\"Agri-Environment Schemes - Higher Level:\")\n",
    "print(f\"Shape: {agri_higher.shape} | Years: {agri_higher['Year'].min():.0f}-{agri_higher['Year'].max():.0f}\")\n",
    "print(f\"Countries: {agri_higher['Country'].unique().tolist()}\")\n",
    "display(agri_higher.head(8))\n",
    "\n",
    "print(f\"\\nEntry Level: {agri_entry.shape} | Years: {agri_entry['Year'].min():.0f}-{agri_entry['Year'].max():.0f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Plants of the Wider Countryside Dataset\n",
    "\n",
    "> **Note**: This dataset uses a non-standard OOXML namespace in its workbook.xml, rendering it unreadable by openpyxl. We implement a custom XML parser to extract the data directly from the xlsx zip archive. This is documented as a data quality challenge.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def parse_plants_xlsx(filepath):\n",
    "    \"\"\"Custom parser for the Plants dataset which uses non-standard OOXML namespace.\"\"\"\n",
    "    with zipfile.ZipFile(filepath) as z:\n",
    "        # Extract shared strings (text values)\n",
    "        shared_strings = []\n",
    "        if 'xl/sharedStrings.xml' in z.namelist():\n",
    "            ss_raw = z.read('xl/sharedStrings.xml').decode('utf-8')\n",
    "            ss_root = ET.fromstring(ss_raw)\n",
    "            for elem in ss_root.iter():\n",
    "                if elem.tag.endswith('}t') or elem.tag == 't':\n",
    "                    shared_strings.append(elem.text if elem.text else '')\n",
    "        \n",
    "        results = {}\n",
    "        for sheet_idx, sheet_name in [(4, 'abundance'), (5, 'species_list')]:\n",
    "            sf = f'xl/worksheets/sheet{sheet_idx}.xml'\n",
    "            if sf not in z.namelist():\n",
    "                continue\n",
    "            content = z.read(sf).decode('utf-8')\n",
    "            sheet_root = ET.fromstring(content)\n",
    "            \n",
    "            rows = []\n",
    "            for row_elem in sheet_root.iter():\n",
    "                if row_elem.tag.endswith('}row'):\n",
    "                    row_data = {}\n",
    "                    for cell in row_elem:\n",
    "                        if cell.tag.endswith('}c'):\n",
    "                            ref = cell.get('r', '')\n",
    "                            col_letter = ''.join(c for c in ref if c.isalpha())\n",
    "                            cell_type = cell.get('t', '')\n",
    "                            val_elem = None\n",
    "                            for child in cell:\n",
    "                                if child.tag.endswith('}v'):\n",
    "                                    val_elem = child\n",
    "                                    break\n",
    "                            if val_elem is not None:\n",
    "                                if cell_type == 's':\n",
    "                                    idx = int(val_elem.text)\n",
    "                                    value = shared_strings[idx] if idx < len(shared_strings) else None\n",
    "                                else:\n",
    "                                    try:\n",
    "                                        value = float(val_elem.text)\n",
    "                                    except ValueError:\n",
    "                                        value = val_elem.text\n",
    "                            else:\n",
    "                                value = None\n",
    "                            row_data[col_letter] = value\n",
    "                    if row_data:\n",
    "                        rows.append(row_data)\n",
    "            results[sheet_name] = rows\n",
    "    return results\n",
    "\n",
    "plants_raw = parse_plants_xlsx(os.path.join(BASE_PATH, FILES['plants']))\n",
    "\n",
    "# Parse abundance data (skip header rows 0-5, data starts at row 5 with headers at row 5)\n",
    "abundance_rows = plants_raw['abundance'][5:]  # Skip metadata rows\n",
    "plants_abundance = pd.DataFrame(abundance_rows)\n",
    "plants_abundance = plants_abundance.rename(columns={'A': 'Habitat', 'B': 'Year', 'C': 'Unsmoothed_Index',\n",
    "                                                      'D': 'Lower_95_CI', 'E': 'Upper_95_CI'})\n",
    "plants_abundance = plants_abundance[['Habitat', 'Year', 'Unsmoothed_Index', 'Lower_95_CI', 'Upper_95_CI']]\n",
    "for col in ['Year', 'Unsmoothed_Index', 'Lower_95_CI', 'Upper_95_CI']:\n",
    "    plants_abundance[col] = pd.to_numeric(plants_abundance[col], errors='coerce')\n",
    "\n",
    "# Parse species list\n",
    "species_rows = plants_raw['species_list'][4:]\n",
    "plants_species = pd.DataFrame(species_rows)\n",
    "plants_species = plants_species.rename(columns={'A': 'Habitat', 'B': 'Species', 'C': 'Common_Name'})\n",
    "plants_species = plants_species[['Habitat', 'Species', 'Common_Name']]\n",
    "\n",
    "print(\"Plants of Wider Countryside - Abundance Index:\")\n",
    "print(f\"Shape: {plants_abundance.shape}\")\n",
    "print(f\"Habitats: {plants_abundance['Habitat'].unique().tolist()}\")\n",
    "print(f\"Year range: {plants_abundance['Year'].min():.0f}-{plants_abundance['Year'].max():.0f}\")\n",
    "display(plants_abundance.head(10))\n",
    "print(f\"\\nSpecies list: {len(plants_species)} species across {plants_species['Habitat'].nunique()} habitats\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Insects of the Wider Countryside (Butterflies) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Sheet 1 contains 7 side-by-side tables. Let's parse them individually.\n",
    "insects_raw = pd.read_excel(os.path.join(BASE_PATH, FILES['insects']), sheet_name='1', header=None)\n",
    "\n",
    "# Table 1: All species (columns 0-4), starts at row 6 with headers, data from row 7\n",
    "# Tables are separated by NaN columns\n",
    "def extract_insect_table(df, col_start, col_end, header_row=6, name=''):\n",
    "    \"\"\"Extract a sub-table from the side-by-side layout.\"\"\"\n",
    "    sub = df.iloc[header_row:, col_start:col_end].reset_index(drop=True)\n",
    "    headers = sub.iloc[0].tolist()\n",
    "    sub = sub.iloc[1:].reset_index(drop=True)\n",
    "    sub.columns = headers\n",
    "    sub = sub.dropna(how='all')\n",
    "    for col in sub.columns:\n",
    "        sub[col] = pd.to_numeric(sub[col], errors='coerce')\n",
    "    return sub\n",
    "\n",
    "# Extract Table 1: All species (cols 0-4)\n",
    "insects_all = extract_insect_table(insects_raw, 0, 5)\n",
    "insects_all.columns = ['Year', 'Unsmoothed', 'Smoothed', 'Lower_CI', 'Upper_CI']\n",
    "insects_all = insects_all.dropna(subset=['Year'])\n",
    "\n",
    "# Extract Table 2: Habitat specialists (cols 6-10)\n",
    "insects_specialist = extract_insect_table(insects_raw, 6, 11)\n",
    "insects_specialist.columns = ['Year', 'Unsmoothed', 'Smoothed', 'Lower_CI', 'Upper_CI']\n",
    "insects_specialist = insects_specialist.dropna(subset=['Year'])\n",
    "\n",
    "# Extract Table 3: Generalists (cols 12-16)\n",
    "insects_generalist = extract_insect_table(insects_raw, 12, 17)\n",
    "insects_generalist.columns = ['Year', 'Unsmoothed', 'Smoothed', 'Lower_CI', 'Upper_CI']\n",
    "insects_generalist = insects_generalist.dropna(subset=['Year'])\n",
    "\n",
    "# Extract Table 4: Farmland (cols 18-23)\n",
    "insects_farmland = extract_insect_table(insects_raw, 18, 24)\n",
    "insects_farmland.columns = ['Year', 'Unsmoothed', 'Smoothed', 'Lower_CI', 'Upper_CI', 'Extra']\n",
    "insects_farmland = insects_farmland.drop(columns=['Extra'], errors='ignore')\n",
    "insects_farmland = insects_farmland.dropna(subset=['Year'])\n",
    "\n",
    "print(\"Insects of Wider Countryside - Butterfly Trends:\")\n",
    "print(f\"All species: {insects_all.shape} | Years: {insects_all['Year'].min():.0f}-{insects_all['Year'].max():.0f}\")\n",
    "print(f\"Habitat specialists: {insects_specialist.shape}\")\n",
    "print(f\"Generalists: {insects_generalist.shape}\")\n",
    "print(f\"Farmland: {insects_farmland.shape}\")\n",
    "display(insects_all.head())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Sheet 3: Individual species abundance trends\n",
    "insects_sp_raw = pd.read_excel(os.path.join(BASE_PATH, FILES['insects']), sheet_name='3', header=None)\n",
    "\n",
    "# Table 1: All species (cols 0-8)\n",
    "sp_data = insects_sp_raw.iloc[8:, 0:9].reset_index(drop=True)\n",
    "sp_data.columns = ['Common_Name', 'Species_Name', 'Type', 'LT_Change', 'LT_Significance',\n",
    "                    'LT_Trend', 'ST_Change', 'ST_Significance', 'ST_Trend']\n",
    "sp_data = sp_data.dropna(subset=['Common_Name'])\n",
    "\n",
    "# Clean [x] values\n",
    "sp_data = sp_data.replace('[x]', np.nan)\n",
    "sp_data['LT_Change'] = pd.to_numeric(sp_data['LT_Change'], errors='coerce')\n",
    "sp_data['ST_Change'] = pd.to_numeric(sp_data['ST_Change'], errors='coerce')\n",
    "\n",
    "print(f\"Individual butterfly species trends: {len(sp_data)} species\")\n",
    "print(f\"Types: {sp_data['Type'].value_counts().to_dict()}\")\n",
    "display(sp_data.head(10))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Initial Observations\n",
    "\n",
    "**Key findings from data exploration:**\n",
    "\n",
    "1. **Temporal Mismatch**: Datasets cover different time ranges â€” Pollinating Insects (1980â€“2024), Habitat Connectivity (1985â€“2012), Agri-Environment (1992â€“2022), Plants (2015â€“2024), Butterflies (1976â€“2024). The overlapping window is limited.\n",
    "\n",
    "2. **Data Quality Issues**:\n",
    "   - Plants xlsx uses non-standard OOXML namespace requiring custom parsing\n",
    "   - Habitat connectivity data contains `[x]` markers for unavailable values\n",
    "   - Multiple datasets have metadata rows that need careful skipping\n",
    "   - Side-by-side table layouts in Insects dataset require custom extraction\n",
    "\n",
    "3. **Scale of Decline**: All-pollinator occupancy dropped from 100 (1980) to ~77 (2024) â€” a 23% decline. Hoverflies show steepest decline (~44%), while wild bees show an increase (~18%).\n",
    "\n",
    "4. **Agri-Environment Schemes**: Higher-level scheme coverage grew substantially (England: 0.175 to 2.25 M hectares), suggesting increasing policy intervention.\n",
    "\n",
    "5. **Feature Availability**: The common time window across most datasets is approximately 1992â€“2012 for the fullest overlap.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning & Pre-processing with AI Optimisation\n",
    "\n",
    "### 4.1 Genetic Algorithm for Optimal Imputation Strategy\n",
    "\n",
    "We use a **Genetic Algorithm (GA)** to optimise the selection of imputation strategies for missing values. Each \"gene\" represents the imputation method for a specific column, and the fitness function evaluates how well the imputed data maintains statistical properties (mean, variance, correlation structure).\n",
    "\n",
    "**Justification**: A GA is appropriate here because:\n",
    "- The search space is discrete (choice of imputation method per column)\n",
    "- Multiple objectives must be balanced (preserving statistical properties)\n",
    "- The optimal combination is not obvious a priori (Brownlee, 2011)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def genetic_algorithm_imputation(df, population_size=20, generations=30, mutation_rate=0.2):\n",
    "    \"\"\"\n",
    "    Genetic Algorithm to find optimal imputation strategy per column.\n",
    "    \n",
    "    Chromosome: List of integers, one per numeric column with missing values.\n",
    "    Gene values: 0=mean, 1=median, 2=linear_interpolation, 3=forward_fill\n",
    "    \n",
    "    Fitness: Minimise distortion of column statistics (variance preservation)\n",
    "    and maximise smoothness of time-series (lower second derivative).\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cols_with_na = [c for c in numeric_cols if df[c].isnull().any()]\n",
    "    \n",
    "    if not cols_with_na:\n",
    "        print(\"No missing values in numeric columns â€” no imputation needed.\")\n",
    "        return df, {}\n",
    "    \n",
    "    n_genes = len(cols_with_na)\n",
    "    methods = ['mean', 'median', 'interpolate', 'ffill']\n",
    "    \n",
    "    # Store original statistics for fitness evaluation\n",
    "    original_stats = {}\n",
    "    for col in cols_with_na:\n",
    "        valid = df[col].dropna()\n",
    "        original_stats[col] = {\n",
    "            'mean': valid.mean(),\n",
    "            'std': valid.std(),\n",
    "            'autocorr': valid.autocorr() if len(valid) > 2 else 0\n",
    "        }\n",
    "    \n",
    "    def apply_imputation(df_in, chromosome):\n",
    "        df_imp = df_in.copy()\n",
    "        for i, col in enumerate(cols_with_na):\n",
    "            method = methods[chromosome[i]]\n",
    "            if method == 'mean':\n",
    "                df_imp[col].fillna(df_imp[col].mean(), inplace=True)\n",
    "            elif method == 'median':\n",
    "                df_imp[col].fillna(df_imp[col].median(), inplace=True)\n",
    "            elif method == 'interpolate':\n",
    "                df_imp[col].interpolate(method='linear', inplace=True)\n",
    "                df_imp[col].fillna(method='bfill', inplace=True)\n",
    "                df_imp[col].fillna(method='ffill', inplace=True)\n",
    "            elif method == 'ffill':\n",
    "                df_imp[col].fillna(method='ffill', inplace=True)\n",
    "                df_imp[col].fillna(method='bfill', inplace=True)\n",
    "        return df_imp\n",
    "    \n",
    "    def fitness(chromosome):\n",
    "        df_imp = apply_imputation(df.copy(), chromosome)\n",
    "        score = 0.0\n",
    "        for i, col in enumerate(cols_with_na):\n",
    "            imp_std = df_imp[col].std()\n",
    "            orig_std = original_stats[col]['std']\n",
    "            if orig_std > 0:\n",
    "                std_ratio = abs(imp_std - orig_std) / orig_std\n",
    "                score -= std_ratio  # Penalise variance distortion\n",
    "            \n",
    "            # Reward smoothness (lower second derivative)\n",
    "            diff2 = df_imp[col].diff().diff().dropna()\n",
    "            smoothness = -diff2.abs().mean() / (df_imp[col].std() + 1e-8)\n",
    "            score += smoothness * 0.1\n",
    "        return score\n",
    "    \n",
    "    # Initialise population\n",
    "    random.seed(42)\n",
    "    population = [[random.randint(0, len(methods)-1) for _ in range(n_genes)] \n",
    "                   for _ in range(population_size)]\n",
    "    \n",
    "    best_chromosome = None\n",
    "    best_fitness = -float('inf')\n",
    "    fitness_history = []\n",
    "    \n",
    "    for gen in range(generations):\n",
    "        # Evaluate fitness\n",
    "        scores = [(chrom, fitness(chrom)) for chrom in population]\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if scores[0][1] > best_fitness:\n",
    "            best_fitness = scores[0][1]\n",
    "            best_chromosome = scores[0][0].copy()\n",
    "        \n",
    "        fitness_history.append(best_fitness)\n",
    "        \n",
    "        # Selection (tournament)\n",
    "        new_pop = [scores[0][0], scores[1][0]]  # Elitism\n",
    "        while len(new_pop) < population_size:\n",
    "            t1, t2 = random.sample(scores, 2)\n",
    "            parent1 = t1[0] if t1[1] > t2[1] else t2[0]\n",
    "            t1, t2 = random.sample(scores, 2)\n",
    "            parent2 = t1[0] if t1[1] > t2[1] else t2[0]\n",
    "            \n",
    "            # Crossover\n",
    "            cx = random.randint(1, n_genes - 1) if n_genes > 1 else 0\n",
    "            child = parent1[:cx] + parent2[cx:]\n",
    "            \n",
    "            # Mutation\n",
    "            for j in range(n_genes):\n",
    "                if random.random() < mutation_rate:\n",
    "                    child[j] = random.randint(0, len(methods)-1)\n",
    "            new_pop.append(child)\n",
    "        \n",
    "        population = new_pop\n",
    "    \n",
    "    # Apply best imputation\n",
    "    result_df = apply_imputation(df, best_chromosome)\n",
    "    best_methods = {cols_with_na[i]: methods[best_chromosome[i]] for i in range(n_genes)}\n",
    "    \n",
    "    print(f\"GA Optimisation Complete ({generations} generations)\")\n",
    "    print(f\"Best fitness: {best_fitness:.4f}\")\n",
    "    print(f\"Optimal imputation methods: {best_methods}\")\n",
    "    \n",
    "    # Plot fitness convergence\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(fitness_history, 'b-', linewidth=2)\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Fitness Score')\n",
    "    plt.title('Genetic Algorithm: Imputation Optimisation Convergence')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return result_df, best_methods\n",
    "\n",
    "print(\"Genetic Algorithm imputation function defined.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Cleaning & Applying GA Imputation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---- Clean Pollinating Insects ----\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEANING: Pollinating Insects\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Missing values before:\\n{poll_yearly.isnull().sum()}\")\n",
    "poll_yearly_clean, poll_methods = genetic_algorithm_imputation(poll_yearly)\n",
    "print(f\"Missing values after:\\n{poll_yearly_clean.isnull().sum()}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---- Clean Habitat Connectivity ----\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEANING: Habitat Connectivity (Butterflies)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Replace [x] markers already done during loading\n",
    "print(f\"Missing values before:\\n{hab_butterfly.isnull().sum()}\")\n",
    "hab_butterfly_clean, hab_methods = genetic_algorithm_imputation(hab_butterfly)\n",
    "print(f\"Missing values after:\\n{hab_butterfly_clean.isnull().sum()}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---- Clean Agri-Environment ----\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEANING: Agri-Environment Schemes\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Aggregate total area across all countries per year for each scheme type\n",
    "agri_higher_total = agri_higher.groupby('Year')['Area_MHa'].sum().reset_index()\n",
    "agri_higher_total.columns = ['Year', 'Higher_Level_Area']\n",
    "\n",
    "agri_entry_total = agri_entry.groupby('Year')['Area_MHa'].sum().reset_index()\n",
    "agri_entry_total.columns = ['Year', 'Entry_Level_Area']\n",
    "\n",
    "# Merge both scheme types\n",
    "agri_combined = pd.merge(agri_higher_total, agri_entry_total, on='Year', how='outer')\n",
    "agri_combined['Total_AE_Area'] = agri_combined['Higher_Level_Area'].fillna(0) + agri_combined['Entry_Level_Area'].fillna(0)\n",
    "agri_combined = agri_combined.sort_values('Year').reset_index(drop=True)\n",
    "\n",
    "print(f\"Combined agri-environment data: {agri_combined.shape}\")\n",
    "print(f\"Year range: {agri_combined['Year'].min():.0f}-{agri_combined['Year'].max():.0f}\")\n",
    "display(agri_combined.head(10))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---- Clean Insects (Butterflies) ----\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEANING: Insects of Wider Countryside\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Missing values before:\\n{insects_all.isnull().sum()}\")\n",
    "insects_all_clean, insects_methods = genetic_algorithm_imputation(insects_all)\n",
    "print(f\"Missing values after:\\n{insects_all_clean.isnull().sum()}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---- Clean Plants ----\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEANING: Plants of Wider Countryside\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Average across habitats per year\n",
    "plants_avg = plants_abundance.groupby('Year')['Unsmoothed_Index'].mean().reset_index()\n",
    "plants_avg.columns = ['Year', 'Plant_Abundance_Index']\n",
    "print(f\"Plants averaged by year: {plants_avg.shape}\")\n",
    "print(f\"Year range: {plants_avg['Year'].min():.0f}-{plants_avg['Year'].max():.0f}\")\n",
    "display(plants_avg)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering & Merged Dataset\n",
    "\n",
    "### 5.1 Creating a Unified Dataset\n",
    "\n",
    "We merge all cleaned datasets into a single time-indexed dataset. The overlapping time range across most datasets is **1992â€“2022**. Where datasets have shorter ranges, we use simulated data based on trend extrapolation and statistical properties.\n",
    "\n",
    "**Justification for simulated data**: The Plants dataset only covers 2015â€“2024 and Habitat Connectivity ends in 2012. To create a coherent merged dataset, we extrapolate missing years using linear regression on available data points, adding Gaussian noise calibrated to observed variance (James et al., 2013).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Simulate/extend data for datasets with shorter time ranges\n",
    "\n",
    "def extrapolate_series(df, year_col, value_col, target_years):\n",
    "    \"\"\"Extrapolate a time series to cover target years using linear trend + noise.\"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    existing = df[[year_col, value_col]].dropna()\n",
    "    X = existing[year_col].values.reshape(-1, 1)\n",
    "    y = existing[value_col].values\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, y)\n",
    "    \n",
    "    residual_std = np.std(y - lr.predict(X))\n",
    "    \n",
    "    all_years = sorted(set(target_years) | set(existing[year_col].values))\n",
    "    result = pd.DataFrame({year_col: all_years})\n",
    "    result = result.merge(existing, on=year_col, how='left')\n",
    "    \n",
    "    missing_mask = result[value_col].isnull()\n",
    "    if missing_mask.any():\n",
    "        predicted = lr.predict(result.loc[missing_mask, year_col].values.reshape(-1, 1))\n",
    "        np.random.seed(42)\n",
    "        noise = np.random.normal(0, residual_std * 0.5, len(predicted))\n",
    "        result.loc[missing_mask, value_col] = predicted + noise\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Target year range\n",
    "target_years = list(range(1992, 2023))\n",
    "\n",
    "# Extend habitat connectivity (ends at 2012)\n",
    "hab_extended = extrapolate_series(hab_butterfly_clean, 'Year', 'Smoothed_Index', target_years)\n",
    "hab_extended = hab_extended.rename(columns={'Smoothed_Index': 'Habitat_Connectivity'})\n",
    "hab_extended = hab_extended[hab_extended['Year'].isin(target_years)]\n",
    "\n",
    "# Extend plants (starts at 2015)\n",
    "plants_extended = extrapolate_series(plants_avg, 'Year', 'Plant_Abundance_Index', target_years)\n",
    "plants_extended = plants_extended[plants_extended['Year'].isin(target_years)]\n",
    "\n",
    "print(\"Extended Habitat Connectivity (simulated post-2012):\")\n",
    "display(hab_extended.tail())\n",
    "print(\"\\nExtended Plants (simulated pre-2015):\")\n",
    "display(plants_extended.head())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build the unified dataset\n",
    "unified = pd.DataFrame({'Year': target_years})\n",
    "\n",
    "# Merge pollinator occupancy\n",
    "unified = unified.merge(poll_yearly_clean[['Year', 'Indicator_Mean_Occupancy']], on='Year', how='left')\n",
    "unified = unified.rename(columns={'Indicator_Mean_Occupancy': 'Pollinator_Index'})\n",
    "\n",
    "# Merge bee and hoverfly separately\n",
    "unified = unified.merge(poll_bees[['Year', 'Bee_Occupancy']], on='Year', how='left')\n",
    "unified = unified.merge(poll_hover[['Year', 'Hoverfly_Occupancy']], on='Year', how='left')\n",
    "\n",
    "# Merge agri-environment\n",
    "unified = unified.merge(agri_combined[['Year', 'Total_AE_Area', 'Higher_Level_Area', 'Entry_Level_Area']], on='Year', how='left')\n",
    "\n",
    "# Merge habitat connectivity (extended)\n",
    "unified = unified.merge(hab_extended[['Year', 'Habitat_Connectivity']], on='Year', how='left')\n",
    "\n",
    "# Merge butterfly abundance (all species)\n",
    "unified = unified.merge(insects_all_clean[['Year', 'Smoothed']].rename(columns={'Smoothed': 'Butterfly_Abundance'}),\n",
    "                         on='Year', how='left')\n",
    "\n",
    "# Merge plants\n",
    "unified = unified.merge(plants_extended[['Year', 'Plant_Abundance_Index']], on='Year', how='left')\n",
    "\n",
    "# Fill any remaining NaN with interpolation\n",
    "unified = unified.sort_values('Year')\n",
    "for col in unified.columns:\n",
    "    if col != 'Year':\n",
    "        unified[col] = unified[col].interpolate(method='linear')\n",
    "        unified[col] = unified[col].fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "print(\"Unified Dataset:\")\n",
    "print(f\"Shape: {unified.shape}\")\n",
    "print(f\"Columns: {unified.columns.tolist()}\")\n",
    "print(f\"Missing values:\\n{unified.isnull().sum()}\")\n",
    "display(unified)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Feature Engineering: Add derived features\n",
    "unified['AE_Change'] = unified['Total_AE_Area'].diff()\n",
    "unified['Pollinator_Change'] = unified['Pollinator_Index'].diff()\n",
    "unified['Pollinator_Lag1'] = unified['Pollinator_Index'].shift(1)\n",
    "unified['Pollinator_Lag2'] = unified['Pollinator_Index'].shift(2)\n",
    "unified['AE_Rolling3'] = unified['Total_AE_Area'].rolling(window=3, min_periods=1).mean()\n",
    "unified['Butterfly_Change'] = unified['Butterfly_Abundance'].diff()\n",
    "unified['Connectivity_Change'] = unified['Habitat_Connectivity'].diff()\n",
    "\n",
    "# Fill NaN from diff/shift operations\n",
    "unified = unified.fillna(method='bfill')\n",
    "\n",
    "print(\"Feature-engineered unified dataset:\")\n",
    "print(f\"Shape: {unified.shape}\")\n",
    "print(f\"New columns: {[c for c in unified.columns if 'Change' in c or 'Lag' in c or 'Rolling' in c]}\")\n",
    "display(unified.describe())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualise the unified dataset\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "\n",
    "axes[0,0].plot(unified['Year'], unified['Pollinator_Index'], 'b-o', markersize=3, label='All Pollinators')\n",
    "axes[0,0].plot(unified['Year'], unified['Bee_Occupancy'], 'g--', alpha=0.7, label='Wild Bees')\n",
    "axes[0,0].plot(unified['Year'], unified['Hoverfly_Occupancy'], 'r--', alpha=0.7, label='Hoverflies')\n",
    "axes[0,0].set_title('Pollinator Occupancy Index')\n",
    "axes[0,0].legend(fontsize=8)\n",
    "axes[0,0].set_ylabel('Index (1980=100)')\n",
    "\n",
    "axes[0,1].plot(unified['Year'], unified['Total_AE_Area'], 'g-o', markersize=3)\n",
    "axes[0,1].set_title('Total Agri-Environment Scheme Area')\n",
    "axes[0,1].set_ylabel('Million Hectares')\n",
    "\n",
    "axes[1,0].plot(unified['Year'], unified['Habitat_Connectivity'], 'purple', linewidth=2)\n",
    "axes[1,0].set_title('Habitat Connectivity (Butterflies)')\n",
    "axes[1,0].set_ylabel('Smoothed Index')\n",
    "\n",
    "axes[1,1].plot(unified['Year'], unified['Butterfly_Abundance'], 'orange', linewidth=2)\n",
    "axes[1,1].set_title('Butterfly Abundance (All Species)')\n",
    "axes[1,1].set_ylabel('Smoothed Index')\n",
    "\n",
    "axes[2,0].plot(unified['Year'], unified['Plant_Abundance_Index'], 'green', linewidth=2)\n",
    "axes[2,0].set_title('Plant Abundance Index')\n",
    "axes[2,0].set_ylabel('Index')\n",
    "\n",
    "# Correlation heatmap\n",
    "corr_cols = ['Pollinator_Index', 'Total_AE_Area', 'Habitat_Connectivity', 'Butterfly_Abundance', 'Plant_Abundance_Index']\n",
    "corr = unified[corr_cols].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='RdYlGn', center=0, ax=axes[2,1], fmt='.2f')\n",
    "axes[2,1].set_title('Feature Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('UK Biodiversity Indicators â€” Unified Dataset Overview', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Supervised Machine Learning â€” Predicting Pollinator Decline\n",
    "\n",
    "### 6.1 Problem Formulation\n",
    "\n",
    "We frame this as a **regression problem**: predicting the Pollinator Occupancy Index based on environmental and biodiversity features.\n",
    "\n",
    "**Target variable**: `Pollinator_Index` (mean occupancy of all pollinators)  \n",
    "**Features**: Agri-environment area, habitat connectivity, butterfly abundance, plant abundance, and lagged/derived features.\n",
    "\n",
    "**Justification**: A regression approach allows us to quantify the magnitude of predicted change, which is more informative for policy-makers than simple classification (Hastie, Tibshirani & Friedman, 2009). We test three algorithms:\n",
    "- **Random Forest**: Robust ensemble method, handles non-linear relationships\n",
    "- **Gradient Boosting**: Sequential boosting for high predictive accuracy\n",
    "- **Support Vector Regression (SVR)**: Effective in high-dimensional, small-sample settings\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Prepare features and target\n",
    "feature_cols = ['Total_AE_Area', 'Higher_Level_Area', 'Entry_Level_Area',\n",
    "                'Habitat_Connectivity', 'Butterfly_Abundance', 'Plant_Abundance_Index',\n",
    "                'AE_Change', 'AE_Rolling3', 'Butterfly_Change', 'Connectivity_Change',\n",
    "                'Pollinator_Lag1', 'Pollinator_Lag2']\n",
    "\n",
    "target_col = 'Pollinator_Index'\n",
    "\n",
    "# Ensure no NaN\n",
    "model_data = unified[['Year'] + feature_cols + [target_col]].dropna()\n",
    "print(f\"Modelling data shape: {model_data.shape}\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "print(f\"Target: {target_col}\")\n",
    "\n",
    "X = model_data[feature_cols].values\n",
    "y = model_data[target_col].values\n",
    "years = model_data['Year'].values\n",
    "\n",
    "# Time-series aware split (80/20)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "years_train, years_test = years[:split_idx], years[split_idx:]\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples (years {years_train[0]:.0f}-{years_train[-1]:.0f})\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples (years {years_test[0]:.0f}-{years_test[-1]:.0f})\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Model Training & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define models with hyperparameter grids\n",
    "models = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 5, 10, None],\n",
    "            'min_samples_split': [2, 3, 5]\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [2, 3, 5],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'SVR': {\n",
    "        'model': SVR(),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'epsilon': [0.01, 0.1, 0.5],\n",
    "            'kernel': ['rbf', 'linear']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "results = {}\n",
    "best_models = {}\n",
    "\n",
    "for name, config in models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Use TimeSeriesSplit for cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        config['model'],\n",
    "        config['params'],\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    if name == 'SVR':\n",
    "        grid.fit(X_train_scaled, y_train)\n",
    "        y_pred = grid.predict(X_test_scaled)\n",
    "    else:\n",
    "        grid.fit(X_train, y_train)\n",
    "        y_pred = grid.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'predictions': y_pred}\n",
    "    best_models[name] = grid.best_estimator_\n",
    "    \n",
    "    print(f\"Best params: {grid.best_params_}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"RÂ²: {r2:.4f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Model Comparison & Predictions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare model performance\n",
    "results_df = pd.DataFrame({name: {k: v for k, v in r.items() if k != 'predictions'} \n",
    "                            for name, r in results.items()}).T\n",
    "print(\"Model Performance Comparison:\")\n",
    "display(results_df)\n",
    "\n",
    "# Find best model\n",
    "best_name = results_df['R2'].idxmax()\n",
    "print(f\"\\nðŸ† Best model: {best_name} (RÂ² = {results_df.loc[best_name, 'R2']:.4f})\")\n",
    "\n",
    "# Visualise predictions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, (name, res) in enumerate(results.items()):\n",
    "    ax = axes[i]\n",
    "    ax.plot(years_test, y_test, 'bo-', label='Actual', markersize=6)\n",
    "    ax.plot(years_test, res['predictions'], 'r^--', label='Predicted', markersize=6)\n",
    "    ax.set_title(f'{name}\\nRÂ²={res[\"R2\"]:.3f}, RMSE={res[\"RMSE\"]:.3f}')\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Pollinator Index')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Supervised ML: Predicted vs Actual Pollinator Index', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Feature importance from best tree-based model\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Random Forest feature importance\n",
    "rf_importance = best_models['Random Forest'].feature_importances_\n",
    "rf_sorted = np.argsort(rf_importance)[::-1]\n",
    "axes[0].barh(range(len(feature_cols)), rf_importance[rf_sorted], color='steelblue')\n",
    "axes[0].set_yticks(range(len(feature_cols)))\n",
    "axes[0].set_yticklabels([feature_cols[i] for i in rf_sorted])\n",
    "axes[0].set_title('Random Forest â€” Feature Importance')\n",
    "axes[0].set_xlabel('Importance')\n",
    "\n",
    "# Gradient Boosting feature importance\n",
    "gb_importance = best_models['Gradient Boosting'].feature_importances_\n",
    "gb_sorted = np.argsort(gb_importance)[::-1]\n",
    "axes[1].barh(range(len(feature_cols)), gb_importance[gb_sorted], color='coral')\n",
    "axes[1].set_yticks(range(len(feature_cols)))\n",
    "axes[1].set_yticklabels([feature_cols[i] for i in gb_sorted])\n",
    "axes[1].set_title('Gradient Boosting â€” Feature Importance')\n",
    "axes[1].set_xlabel('Importance')\n",
    "\n",
    "plt.suptitle('Feature Importance for Pollinator Index Prediction', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features (Gradient Boosting):\")\n",
    "for i in gb_sorted[:5]:\n",
    "    print(f\"  {feature_cols[i]}: {gb_importance[i]:.4f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Unsupervised Machine Learning â€” Clustering Species by Trend Patterns\n",
    "\n",
    "### 7.1 Approach\n",
    "\n",
    "We apply **K-Means clustering** to group butterfly species based on their long-term and short-term abundance change patterns. This helps identify which groups of species are most affected and could inform targeted conservation strategies.\n",
    "\n",
    "We also use **PCA** for dimensionality reduction and visualisation.\n",
    "\n",
    "**Justification**: K-Means is appropriate for identifying natural groupings in species data (MacQueen, 1967). PCA provides interpretable 2D visualisation of high-dimensional species trend data (Jolliffe, 2002).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Prepare species-level data for clustering\n",
    "species_data = sp_data[['Common_Name', 'Type', 'LT_Change', 'ST_Change']].dropna()\n",
    "\n",
    "# Encode species type\n",
    "le = LabelEncoder()\n",
    "species_data['Type_Encoded'] = le.fit_transform(species_data['Type'])\n",
    "\n",
    "# Clustering features\n",
    "cluster_features = ['LT_Change', 'ST_Change', 'Type_Encoded']\n",
    "X_cluster = species_data[cluster_features].values\n",
    "\n",
    "# Scale for clustering\n",
    "scaler_cluster = StandardScaler()\n",
    "X_cluster_scaled = scaler_cluster.fit_transform(X_cluster)\n",
    "\n",
    "print(f\"Species for clustering: {len(species_data)}\")\n",
    "print(f\"Features: {cluster_features}\")\n",
    "print(f\"Type distribution: {species_data['Type'].value_counts().to_dict()}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Finding Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Elbow method and Silhouette analysis\n",
    "K_range = range(2, 8)\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(X_cluster_scaled)\n",
    "    inertias.append(km.inertia_)\n",
    "    silhouettes.append(silhouette_score(X_cluster_scaled, labels))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(K_range, inertias, 'bo-', linewidth=2)\n",
    "axes[0].set_xlabel('Number of Clusters (K)')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].set_title('Elbow Method')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(K_range, silhouettes, 'rs-', linewidth=2)\n",
    "axes[1].set_xlabel('Number of Clusters (K)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Analysis')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "optimal_k = K_range[np.argmax(silhouettes)]\n",
    "print(f\"Optimal K (highest silhouette): {optimal_k} (score: {max(silhouettes):.4f})\")\n",
    "\n",
    "plt.suptitle('K-Means: Determining Optimal Number of Clusters', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Clustering Results & PCA Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Apply K-Means with optimal K\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "species_data['Cluster'] = kmeans.fit_predict(X_cluster_scaled)\n",
    "\n",
    "# PCA for visualisation\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_cluster_scaled)\n",
    "\n",
    "print(f\"PCA explained variance: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total explained: {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# PCA scatter coloured by cluster\n",
    "scatter = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                           c=species_data['Cluster'], cmap='viridis', \n",
    "                           s=80, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "axes[0].set_title('PCA Visualisation of Species Clusters')\n",
    "plt.colorbar(scatter, ax=axes[0], label='Cluster')\n",
    "\n",
    "# Annotate some species\n",
    "for _, row in species_data.iterrows():\n",
    "    idx = species_data.index.get_loc(_)\n",
    "    if abs(X_pca[idx, 0]) > 1.5 or abs(X_pca[idx, 1]) > 1.5:\n",
    "        axes[0].annotate(row['Common_Name'], (X_pca[idx, 0], X_pca[idx, 1]),\n",
    "                         fontsize=7, alpha=0.8)\n",
    "\n",
    "# Cluster means\n",
    "cluster_summary = species_data.groupby('Cluster').agg({\n",
    "    'LT_Change': 'mean',\n",
    "    'ST_Change': 'mean',\n",
    "    'Common_Name': 'count',\n",
    "    'Type': lambda x: x.value_counts().index[0]\n",
    "}).rename(columns={'Common_Name': 'Count', 'Type': 'Dominant_Type'})\n",
    "\n",
    "axes[1].barh(range(len(cluster_summary)), cluster_summary['LT_Change'], \n",
    "             color=['steelblue', 'coral', 'green', 'purple'][:len(cluster_summary)],\n",
    "             alpha=0.7, label='Long-term change')\n",
    "axes[1].set_yticks(range(len(cluster_summary)))\n",
    "axes[1].set_yticklabels([f'Cluster {i}\\n(n={cluster_summary.iloc[i][\"Count\"]:.0f})' \n",
    "                          for i in range(len(cluster_summary))])\n",
    "axes[1].set_xlabel('Mean Change (%)')\n",
    "axes[1].set_title('Mean Long-Term Change by Cluster')\n",
    "axes[1].axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.suptitle('K-Means Clustering of Butterfly Species by Trend Patterns', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCluster Summary:\")\n",
    "display(cluster_summary)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Detailed cluster membership\n",
    "for cluster_id in sorted(species_data['Cluster'].unique()):\n",
    "    members = species_data[species_data['Cluster'] == cluster_id]\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Cluster {cluster_id} ({len(members)} species)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Mean LT Change: {members['LT_Change'].mean():.1f}%\")\n",
    "    print(f\"Mean ST Change: {members['ST_Change'].mean():.1f}%\")\n",
    "    print(f\"Types: {members['Type'].value_counts().to_dict()}\")\n",
    "    print(f\"Species: {', '.join(members['Common_Name'].tolist())}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation & Validation\n",
    "\n",
    "### 8.1 Cross-Validation with TimeSeriesSplit\n",
    "\n",
    "We use **TimeSeriesSplit** cross-validation, which respects the temporal ordering of the data â€” a critical requirement for time-series modelling (Bergmeir & BenÃ­tez, 2012). Standard k-fold would allow future data to inform predictions about the past, leading to overly optimistic estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Time-series cross-validation for the best model\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "best_model = best_models[best_name]\n",
    "\n",
    "if best_name == 'SVR':\n",
    "    X_all = scaler.transform(model_data[feature_cols].values)\n",
    "else:\n",
    "    X_all = model_data[feature_cols].values\n",
    "\n",
    "y_all = model_data[target_col].values\n",
    "\n",
    "cv_scores_mae = cross_val_score(best_model, X_all, y_all, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "cv_scores_r2 = cross_val_score(best_model, X_all, y_all, cv=tscv, scoring='r2')\n",
    "cv_scores_rmse = cross_val_score(best_model, X_all, y_all, cv=tscv, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "print(f\"Cross-Validation Results ({best_name}) â€” 5-fold TimeSeriesSplit:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"MAE:  {-cv_scores_mae.mean():.4f} Â± {cv_scores_mae.std():.4f}\")\n",
    "print(f\"RMSE: {-cv_scores_rmse.mean():.4f} Â± {cv_scores_rmse.std():.4f}\")\n",
    "print(f\"RÂ²:   {cv_scores_r2.mean():.4f} Â± {cv_scores_r2.std():.4f}\")\n",
    "\n",
    "# Visualise CV folds\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "folds = list(tscv.split(X_all))\n",
    "for i, (train_idx, test_idx) in enumerate(folds):\n",
    "    ax.scatter(train_idx, [i+1]*len(train_idx), c='steelblue', s=5, label='Train' if i==0 else None)\n",
    "    ax.scatter(test_idx, [i+1]*len(test_idx), c='coral', s=15, label='Test' if i==0 else None)\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('CV Fold')\n",
    "ax.set_title('TimeSeriesSplit Cross-Validation Folds')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_model, X_all, y_all, \n",
    "    train_sizes=np.linspace(0.3, 1.0, 8),\n",
    "    cv=TimeSeriesSplit(n_splits=3),\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "train_mean = -train_scores.mean(axis=1)\n",
    "train_std = train_scores.std(axis=1)\n",
    "val_mean = -val_scores.mean(axis=1)\n",
    "val_std = val_scores.std(axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, 'b-o', label='Training MSE')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "plt.plot(train_sizes, val_mean, 'r-o', label='Validation MSE')\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title(f'Learning Curves â€” {best_name}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"If training and validation curves converge: model generalises well (low variance).\")\n",
    "print(\"If gap remains: model may overfit â€” more data or regularisation needed.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Residuals for the best model\n",
    "if best_name == 'SVR':\n",
    "    y_pred_best = best_model.predict(X_test_scaled)\n",
    "else:\n",
    "    y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "residuals = y_test - y_pred_best\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Residuals vs Predicted\n",
    "axes[0].scatter(y_pred_best, residuals, c='steelblue', edgecolors='black', alpha=0.7)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--')\n",
    "axes[0].set_xlabel('Predicted Values')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "axes[0].set_title('Residuals vs Predicted')\n",
    "\n",
    "# Residual distribution\n",
    "axes[1].hist(residuals, bins=8, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Residual Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Residual Distribution')\n",
    "\n",
    "# Q-Q plot\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[2])\n",
    "axes[2].set_title('Q-Q Plot of Residuals')\n",
    "\n",
    "plt.suptitle(f'Residual Analysis â€” {best_name}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Residual Mean: {residuals.mean():.4f} (should be ~0)\")\n",
    "print(f\"Residual Std: {residuals.std():.4f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Critical Evaluation & Conclusions\n",
    "\n",
    "### 9.1 Summary of Findings\n",
    "\n",
    "1. **Pollinator Decline Confirmed**: All-pollinator occupancy has declined ~23% since 1980. Hoverflies show the most severe decline (~44%), while wild bee occupancy has actually increased (~18%).\n",
    "\n",
    "2. **Agri-Environment Schemes**: Significant increase in scheme coverage (particularly England), but the relationship with pollinator recovery is complex â€” the correlation analysis shows this requires careful interpretation.\n",
    "\n",
    "3. **Species Clustering**: K-Means clustering identified distinct groups of butterfly species experiencing different trend trajectories, which can inform targeted conservation priorities.\n",
    "\n",
    "4. **Predictive Modelling**: The supervised ML models achieved reasonable predictive performance, with lagged pollinator values and habitat connectivity emerging as the most important predictors.\n",
    "\n",
    "### 9.2 Model Limitations\n",
    "\n",
    "1. **Small Sample Size**: With ~30 yearly data points in the common time range, all models face limited training data. This is a fundamental constraint of ecological monitoring datasets.\n",
    "\n",
    "2. **Simulated Data**: The extrapolation of Plants and Habitat Connectivity data introduces uncertainty. While calibrated to observed variance, simulated values should not be treated with the same confidence as measured data.\n",
    "\n",
    "3. **Temporal Auto-correlation**: Time-series data inherently has auto-correlation, which can inflate apparent model performance. We mitigated this with TimeSeriesSplit, but the small sample size limits the number of meaningful folds.\n",
    "\n",
    "4. **Confounding Variables**: Many factors affecting biodiversity are not captured in these datasets (climate change, land-use change, pesticide usage, disease). The models capture correlations, not necessarily causal relationships.\n",
    "\n",
    "5. **Aggregation Bias**: Aggregating species or country-level data into UK-wide indices may mask important regional or species-specific trends.\n",
    "\n",
    "### 9.3 Recommendations for Bee Positive\n",
    "\n",
    "1. **Target Hoverfly Conservation**: The severe decline in hoverfly occupancy (~44%) warrants specific policy attention.\n",
    "\n",
    "2. **Integrate More Data**: Future models should incorporate climate data, pesticide usage, and land-use maps for stronger predictive power.\n",
    "\n",
    "3. **Regional Modelling**: Country-level or regional models would provide more actionable insights than UK-wide aggregates.\n",
    "\n",
    "4. **Longitudinal Monitoring**: Expanding the Plants monitoring scheme (currently only 2015â€“2024) would significantly improve model reliability.\n",
    "\n",
    "5. **Causal Inference**: Moving beyond correlation to causal models (e.g., Bayesian networks, instrumental variable analysis) would better support policy evaluation.\n",
    "\n",
    "### 9.4 References\n",
    "\n",
    "- Bergmeir, C. & BenÃ­tez, J.M. (2012). On the use of cross-validation for time series predictor evaluation. *Information Sciences*, 191, 192-213.\n",
    "- Brownlee, J. (2011). *Clever Algorithms: Nature-Inspired Programming Recipes*. Lulu.com.\n",
    "- Hastie, T., Tibshirani, R. & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer.\n",
    "- James, G., Witten, D., Hastie, T. & Tibshirani, R. (2013). *An Introduction to Statistical Learning*. Springer.\n",
    "- Jolliffe, I.T. (2002). *Principal Component Analysis*. 2nd edition. Springer.\n",
    "- MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. *Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability*, 1, 281-297.\n",
    "- Soldaat, L.L. et al. (2017). A Monte Carlo method to account for sampling error in multi-species indicators. *Ecological Indicators*, 81, 340-347.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}